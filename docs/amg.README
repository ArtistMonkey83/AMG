
Code Description

A. General description:

AMG is a parallel algebraic multigrid solver for linear systems arising from
problems on unstructured grids.

See the following papers for details on the algorithm and its parallel
implementation/performance:

Van Emden Henson and Ulrike Meier Yang, "BoomerAMG: A Parallel Algebraic
Multigrid Solver and Preconditioner", Appl. Num. Math. 41 (2002),
pp. 155-177. Also available as LLNL technical report UCRL-JC-141495.

Hans De Sterck, Ulrike Meier Yang and Jeffrey Heys, "Reducing Complexity in
Parallel Algebraic Multigrid Preconditioners", SIAM Journal on Matrix Analysis
and Applications 27 (2006), pp. 1019-1039. Also available as LLNL technical
reports UCRL-JRNL-206780.

Hans De Sterck, Robert D. Falgout, Josh W. Nolting and Ulrike Meier Yang, 
"Distance-Two Interpolation for Parallel Algebraic Multigrid", Numerical 
Linear Algebra with Applications 15 (2008), pp. 115-139. Also available as 
LLNL technical report UCRL-JRNL-230844.

U. M. Yang, "On Long Range Interpolation Operators for Aggressive Coarsening",
Numer. Linear Algebra Appl.,  17 (2010), pp. 453-472. LLNL-JRNL-417371.

A. H. Baker, R. D. Falgout, T. V. Kolev, and U. M. Yang, "Multigrid Smoothers 
for Ultraparallel Computing", SIAM J. Sci. Comput., 33 (2011), pp. 2864-2887. 
LLNL-JRNL-473191.

J. Park, M. Smelyanskiy, u. M. Yang, D. Mudigere, and P. Dubey, "High-
Performance Algebraic Multigrid Solver Optimized for Multi-Core Based 
Distributed Parallel Systems", Proceddings of the International Conference
for High Performance Computing, Networking, Storage and Analysis 2015 (SC15),
LLNL-CONF-669961.


The driver provided with AMG builds linear systems for various 
3-dimensional problems, which are described in Section D.

To determine when the solver has converged, the driver uses the
relative-residual stopping criteria,

  ||r_k||_2 / ||b||_2 < tol

with tol = 10^-6.

B. Coding:

AMG is written in ISO-C.  It is an SPMD code which uses MPI.  Parallelism is
achieved by data decomposition.  The driver provided with AMG achieves this
decomposition by simply subdividing the grid into logical P x Q x R (in 3D)
chunks of equal size.

C. Parallelism:

AMG is a highly synchronous code.  The communications and computations
patterns exhibit the surface-to-volume relationship common to many parallel
scientific codes.  Hence, parallel efficiency is largely determined by the size
of the data "chunks" mentioned above, and the speed of communications and
computations on the machine.  AMG is also memory-access bound, doing only
about 1-2 computations per memory access, so memory-access speeds will also have
a large impact on performance.

D. Test problems

Problem 1 (default): The default problem is a Laplace type problem on a
cube with a 27-point stencil. This problem should be scaled up to fill
the entire memory of the machine.
This problem is solved with AMG-PCG.
Suggestions for test runs are given in Section "Suggested Test Runs".

Problem 2 (-problem 2): Simulates a non-linear time-dependent problem.
This problem also applies to 3-dimensional linear systems with 27-point
stencils on a cube. It runs over 6 time steps. At the beginning of each
time step it sets up the preconditioner and then solvers the problems
5 times within each time step, reusing the preconditioner, but making
small changes to the actual matrix and right hand side.
This problem is solved with AMG-GMRES.
Note that this problem needs to not be shortcut in any way by simplifying
the equations, since the operations used simulate an actual code and
were specifically chosen to evaluate performance.
This problem should fill about 5-10% of the total memory of the machine.
Suggestions for test runs are given in Section "Suggested Test Runs".

%==========================================================================
%==========================================================================

NOTE: The AMG code is derived directly from the hypre library, a large
linear solver library that is being developed in the Center for Applied
Scientific Computing (CASC) at LLNL.

%==========================================================================
%==========================================================================

Building the Code

AMG uses a simple Makefile system for building the code.  All compiler and
link options are set by modifying the file 'amg2013/Makefile.include'
appropriately.  This file is then included in each of the following makefiles:

  krylov/Makefile
  IJ_mv/Makefile
  parcsr_ls/Makefile
  parcsr_mv/Makefile
  seq_mv/Makefile
  test/Makefile
  utilities/Makefile

To build the code, first modify the 'Makefile.include' file appropriately, then
type (in the amg2013 directory)

  make

Other available targets are

  make clean        (deletes .o files)
  make veryclean    (deletes .o files, libraries, and executables)

To configure the code to run with:

1 - MPI only , add '-DTIMER_USE_MPI' to the 'INCLUDE_CFLAGS' line 
    in the 'Makefile.include' file and use a valid MPI.
2 - OpenMP with MPI, add vendor dependent compilation flag for OMP
    and add '-DTIMER_USE_MPI -DHYPRE_USING_OPENMP'
3 - for additional optimzations in MPI and OpenMP, add also
    '-DHYPRE_USING_PERSISTENT_COMM' (for MPI) and
    '-DHYPRE_CONCURRENT_HOPSCOTCH' (for OpenMP) to 'INCLUDE_CFLAGS'
4 - to be able to solve problems that are larger than 2^31-1,
    add '-DHYPRE_BIGINT'

%==========================================================================
%==========================================================================

Optimization and Improvement Challenges

This code is memory-access bound.  We believe it would be very difficult to
obtain "good" cache reuse with an optimized version of the code.

%==========================================================================
%==========================================================================

Parallelism and Scalability Expectations

Previous versions of AMG have been run on the following platforms:

  BG/Q                 - up to over 1,000,000 MPI processes
  BG/P                 - up to 125,000 MPI processes
  and more

Consider increasing both problem size and number of processors in tandem.
On scalable architectures, time-to-solution for AMG2013 will initially
increase, then it will level off at a modest numbers of processors,
remaining roughly constant for larger numbers of processors.  Iteration
counts will also increase slightly for small to modest sized problems,
then level off at a roughly constant number for larger problem sizes.

For example, we get the following timing results (in seconds) for a 3D Laplace
problem with cx = cy = cz = 1.0, distributed on a logical P x Q x R processor 
topology, with fixed local problem size per process given as 40 x 40 x 40:

  P x Q x R     procs    solver similar to solver 0
  ---------------------------------------------------------------
  16x16x16       4096          5.75
  20x20x20       8000          6.88
  32x32x32      32768          8.11
  44x44x44      91125         10.48
  50x50x50     125000         10.54

These results were obtained on BG/P using the assumed partition option
-DHYPRE_NO_GLOBAL_PARTITION and -DHYPRE_LONG_LONG.

%==========================================================================
%==========================================================================

Running the Code

The driver for AMG2013 is called `amg2013', and is located in the amg2013/test
subdirectory.  Type

   mpirun -np 1 amg2013 -help

to get usage information.  This prints out the following:

Usage: amg2013 [<options>]
 
  -in <filename> : input file (default is `sstruct.in.AMG.FD')
 
  -P <Px> <Py> <Pz>   : define processor topology per part
                        Note that for test problem 1, which has 8 parts
			this leads to 8*Px*Py*Pz MPI processes!
			For all other test problems, the total amount of
			MPI processes is Px*Py*Pz.

  -n <nx> <ny> <nz>   : define size per processor for problems on cube

  -problem <p>        : <p> needs to be 1 (default) or 2

  -printstats         : print out detailed info on AMG preconditioner
			and number of iterations
 
  -printallstats      : print out detailed info on AMG preconditioner,
			number of iterations and residual norms for
			each iteration
 
  -printsystem        : print out the system
 

All of the arguments are optional.  The most important option for the AMG
compact application is the `-P' option. It specifies the MPI process topology 
on which to run.  

The `-n' option allows one to specify the local problem size per MPI process, 
leading to a global problem size of <Px>*<nx> by <Py>*<ny> by <Pz>*<nz>.

%==========================================================================
%==========================================================================

Timing Issues

If using MPI, the whole code is timed using the MPI timers.  If not using MPI,
standard system timers are used.  Timing results are printed to standard out,
and are divided into "Setup Phase" times and "Solve Phase" times for Problem
1. For problem 2, the complete time-dependent loop is timed including
various setups and solves.

%==========================================================================
%==========================================================================

Memory Needed

AMG's memory needs are somewhat complicated to describe.  They are very
dependent on the type of problem solved and the AMG options used.  
When turning on the '-printstats' option, memory complexities <mc> are displayed, 
which are defined by the sum of nonzeros of all matrices (both system 
matrices and interpolation matrices on all levels) divided by the number of 
nonzeros of the original matrix, i.e. at least about <mc> times as much space is
needed.  However this does not include memory needed for communication, vectors,
auxiliary computations, etc.

%==========================================================================
%==========================================================================

About the Data

AMG generates its own linear system, the size of which can be controlled from
the command line.

%==========================================================================
%==========================================================================

Expected Results

Consider the following run, which was compiled using options -DTIMER_USE_MPI -DHYPRE_USING_OPENMP,
linking with OpenMP and setting OMP_NUM_THREADS to 2:

  mpirun -np 4 amg -P 2 2 1 -n 50 50 50 -printstats

This is what AMG prints out:

Running with these driver parameters:
  solver ID    = 1

  Laplacian_27pt:
    (Nx, Ny, Nz) = (100, 100, 50)
    (Px, Py, Pz) = (2, 2, 1)

=============================================
Generate Matrix:
=============================================
Spatial Operator:
  wall clock time = 0.077136 seconds
  wall MFLOPS     = 0.000000
  cpu clock time  = 0.200000 seconds
  cpu MFLOPS      = 0.000000

  RHS vector has unit components
  Initial guess is 0
=============================================
IJ Vector Setup:
=============================================
RHS and Initial Guess:
  wall clock time = 0.001619 seconds
  wall MFLOPS     = 0.000000
  cpu clock time  = 0.000000 seconds
  cpu MFLOPS      = 0.000000

Solver: AMG-PCG
HYPRE_ParCSRPCGGetPrecond got good precond


 Num MPI tasks = 4  Num OpenMP threads = 3


BoomerAMG SETUP PARAMETERS:

 Max levels = 25
 Num levels = 5

 Strength Threshold = 0.250000
 Interpolation Truncation Factor = 0.000000
 Maximum Row Sum Threshold for Dependency Weakening = 0.900000

 Coarsening Type = PMIS 

 No. of levels of aggressive coarsening: 1

 Interpolation on agg. levels= multipass interpolation
 measures are determined locally


 No global partition option chosen.

 Interpolation = extended+i interpolation

Operator Matrix Information:

            nonzero         entries per row        row sums
lev   rows  entries  sparse  min  max   avg       min         max
===================================================================
 0  500000 13142992  0.000     8   27  26.3   0.000e+00   1.900e+01
 1    7046   174928  0.004     5   42  24.8  -7.567e-13   2.883e+02
 2    1265   106427  0.067    18  142  84.1  -5.451e-13   4.359e+02
 3     170    13374  0.463    31  140  78.7   8.256e+01   6.706e+02
 4      17      287  0.993    16   17  16.9   2.751e+02   1.063e+03


Interpolation Matrix Information:
                 entries/row    min     max         row sums
lev  rows cols    min max     weight   weight     min       max 
=================================================================
 0 500000 x 7046    1  10   2.262e-03 9.985e-01 2.119e-01 1.000e+00
 1  7046 x 1265    1   8   4.994e-03 6.801e-01 3.312e-01 1.000e+00
 2  1265 x 170     1   8   2.028e-03 7.890e-01 8.586e-02 1.000e+00
 3   170 x 17      0   8  -4.880e-02 1.668e-01 0.000e+00 1.000e+00


     Complexity:    grid = 1.016996
                operator = 1.022447
                memory = 1.090040




BoomerAMG SOLVER PARAMETERS:

  Maximum number of cycles:         1 
  Stopping Tolerance:               0.000000e+00 
  Cycle type (1 = V, 2 = W, etc.):  1

  Relaxation Parameters:
   Visiting Grid:                     down   up  coarse
            Number of sweeps:            2    2     1 
   Type 0=Jac, 3=hGS, 6=hSGS, 9=GE:     18   18    18 
   Point types, partial sweeps (1=C, -1=F):
                  Pre-CG relaxation (down):   0   0
                   Post-CG relaxation (up):   0   0
                             Coarsest grid:   0

=============================================
Problem 1: AMG Setup Time:
=============================================
PCG Setup:
  wall clock time = 0.252805 seconds
  wall MFLOPS     = 0.000000
  cpu clock time  = 0.680000 seconds
  cpu MFLOPS      = 0.000000


nnz_AP / Setup Phase Time: 5.666973e+07

=============================================
Problem 1: AMG-PCG Solve Time:
=============================================
PCG Solve:
  wall clock time = 1.282640 seconds
  wall MFLOPS     = 0.000000
  cpu clock time  = 3.780000 seconds
  cpu MFLOPS      = 0.000000


Iterations = 20
Final Relative Residual Norm = 7.979902e-09

%==========================================================================
%==========================================================================

Figure of Merit

The figures of merit (FOMs) includes wall clock times of portions of
the code (AMG setup, AMG-PCG or AMG-GMRES solve phase, or a combination
of those), number of iterations, number of time steps as well as the total
number of nonzeros for all system matrices and interpolation operators on
all levels of AMG, nnz_AP.

%==========================================================================
%==========================================================================

Suggested Test Runs

1. For Problem 1, the problem needs to be scaled to a size, for which total
memory use is about

mpirun -np <px*py*pz> amg2013 -laplace -n 40 40 40 -P px py pz

This will generate a problem with 64,000 grid points per MPI process
with a domain of the size 40*px x 40*py x 40*pz .

mpirun -np <px*py*pz> amg2013 -laplace -n 80 80 80 -P px py pz

This will generate a problem with 512,000 grid points per MPI process
with a domain of the size 80*px x 80*py x 80*pz .

%==========================================================================
%==========================================================================

For further information on AMG contact
Ulrike Yang
ph: (925)422-2850
email: umyang@llnl.gov

%==========================================================================
%==========================================================================

Release and Modification Record

LLNL code release number: LLNL-xxxxx

See the files COPYRIGHT and LICENSE for a complete copyright notice, 
additional contact information, disclaimer and license.
